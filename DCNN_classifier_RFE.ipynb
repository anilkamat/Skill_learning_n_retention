{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qpaapj7Z9nve"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, Conv1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Dense\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import L1L2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses, regularizers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, auc, confusion_matrix, roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(data):\n",
        "    min = np.min(data, axis=0)\n",
        "    max = np.max(data, axis=0)\n",
        "    data = (data - min) / (max - min)\n",
        "    return data\n",
        "\n",
        "def create_model(filter_sz=64, kernel_sz1=3, kernel_sz2=3, dropout=0.4, input_shape=[None, 1]):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=filter_sz, kernel_size=kernel_sz1, strides=1, activation='relu', input_shape=input_shape, name='L1'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Conv1D(filters=filter_sz, kernel_size=kernel_sz2, strides=1, activation='relu', name='L2'))\n",
        "    model.add(Conv1D(filters=32, kernel_size=1, strides=1, activation='relu', name='L3'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid', name='classification'))\n",
        "    adam = tf.keras.optimizers.Adam(learning_rate=1.e-04)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    area = auc(fpr, tpr)\n",
        "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
        "    ix = np.argmax(gmeans)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, (y_pred >= thresholds[ix])).ravel()\n",
        "    N = tn + fp + fn + tp\n",
        "    S = (tp + fn) / N\n",
        "    P = (tp + fp) / N\n",
        "    MCC = ((tp / N) - S * P) / np.sqrt(P * S * (1 - S) * (1 - P))\n",
        "    accuracy = (tp + tn) / N\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    return accuracy, area, MCC, sensitivity, specificity\n",
        "\n",
        "def cross_val_cumulative_accuracy(X, y, folds=5):\n",
        "    kf = StratifiedKFold(n_splits=folds, shuffle=False)\n",
        "    y_true, y_pred = np.array([]), np.array([])\n",
        "    _, n = X.shape\n",
        "    filter_size = 64\n",
        "\n",
        "    if n > 12:\n",
        "        kernel_sz1 = 5\n",
        "        kernel_sz2 = 7\n",
        "    elif n >= 5 and n <= 12: \n",
        "        kernel_sz1 = n - 2\n",
        "        kernel_sz2 = 3\n",
        "    else:\n",
        "        kernel_sz1 = 1\n",
        "        kernel_sz2 = 1\n",
        "\n",
        "    for train_index, test_index in kf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "        \n",
        "        model = create_model(filter_sz=filter_size, kernel_sz1=kernel_sz1, kernel_sz2=kernel_sz2, input_shape=[X_train.shape[1], 1])\n",
        "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2000, batch_size=16, shuffle=True, verbose=0, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)])\n",
        "        \n",
        "        y_pred_fold = model.predict(X_test).flatten()\n",
        "        y_true = np.append(y_true, y_test)\n",
        "        y_pred = np.append(y_pred, y_pred_fold)\n",
        "    \n",
        "    return compute_metrics(y_true, y_pred)\n",
        "\n",
        "def recursive_feature_elimination(X, y, min_features=1, connectivity_list=None, output_dir=None, st_num=None):\n",
        "    n_features = X.shape[1]\n",
        "    feature_indices = list(range(n_features))\n",
        "    print(feature_indices)\n",
        "    selected_features = []\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    metrics_list = []\n",
        "    feature_sets = []\n",
        "    \n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = \"Feature Sets\"\n",
        "    ws.append([\"Set\", \"Accuracy\", \"AUC\", \"MCC\", \"Sensitivity\", \"Specificity\"] + [f\"Feature_{i}\" for i in range(n_features)])\n",
        "    \n",
        "    set_counter = 1\n",
        "    \n",
        "    while len(feature_indices) >= min_features:\n",
        "        accuracy, auc_score, mcc, sensitivity, specificity = cross_val_cumulative_accuracy(X_scaled[:, feature_indices], y)\n",
        "        metrics_list.append((accuracy, auc_score, mcc, sensitivity, specificity))\n",
        "        feature_sets.append(feature_indices.copy())\n",
        "        \n",
        "        print(f\"\\nCurrent number of features: {len(feature_indices)}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, MCC: {mcc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
        "        print(\"Current feature set:\")\n",
        "        for idx in feature_indices:\n",
        "            if connectivity_list:\n",
        "                print(f\"  Index {idx}: {connectivity_list[idx]}\")\n",
        "            else:\n",
        "                print(f\"  Index {idx}\")\n",
        "        \n",
        "        # Write to Excel\n",
        "        excel_row = [set_counter, accuracy, auc_score, mcc, sensitivity, specificity] + [''] * n_features\n",
        "        for idx in feature_indices:\n",
        "            excel_row[idx + 6] = connectivity_list[idx] if connectivity_list else idx\n",
        "        ws.append(excel_row)\n",
        "        set_counter += 1\n",
        "        \n",
        "        if len(feature_indices) == min_features:\n",
        "            break\n",
        "        \n",
        "        feature_importances = []\n",
        "        for i in feature_indices:\n",
        "            temp_features = [f for f in feature_indices if f != i] # Removes one features at at time and check the accuracy\n",
        "            if temp_features:  # Only compute metrics if there's at least one feature left\n",
        "                temp_metrics = cross_val_cumulative_accuracy(X_scaled[:, temp_features], y)\n",
        "                feature_importances.append((i, temp_metrics[0]))  # Using accuracy for feature importance\n",
        "        \n",
        "        if feature_importances:\n",
        "            worst_feature, _ = max(feature_importances, key=lambda x: x[1])\n",
        "            feature_indices.remove(worst_feature)  # removes the feature that has least contribution/effect on the accuray\n",
        "            selected_features.append(worst_feature)\n",
        "            \n",
        "            if connectivity_list:\n",
        "                print(f\"Removed feature {worst_feature}: {connectivity_list[worst_feature]}\")\n",
        "            else:\n",
        "                print(f\"Removed feature {worst_feature}\")\n",
        "        else:\n",
        "            break  # Stop if we can't remove any more features\n",
        "    \n",
        "    # Find the point where accuracy is highest\n",
        "    best_metrics_index = max(range(len(metrics_list)), key=lambda i: metrics_list[i][0])  # Using accuracy for best set\n",
        "    best_feature_set = feature_sets[best_metrics_index]\n",
        "    best_metrics = metrics_list[best_metrics_index]\n",
        "    \n",
        "    print(\"\\nMetrics for each feature set:\")\n",
        "    for i, (metrics, feat_set) in enumerate(zip(metrics_list, feature_sets)):\n",
        "        print(f\"Features: {len(feat_set)}, Accuracy: {metrics[0]:.4f}, AUC: {metrics[1]:.4f}, MCC: {metrics[2]:.4f}, Sensitivity: {metrics[3]:.4f}, Specificity: {metrics[4]:.4f}\")\n",
        "        if i == best_metrics_index:\n",
        "            print(\"^ Best performing set\")\n",
        "            print(\"Best feature set:\")\n",
        "            for idx in feat_set:\n",
        "                if connectivity_list:\n",
        "                    print(f\"  Index {idx}: {connectivity_list[idx]}\")\n",
        "                else:\n",
        "                    print(f\"  Index {idx}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Best Feature Set: {best_feature_set}\")\n",
        "    if connectivity_list:\n",
        "        print(\"Best Feature Set Connectivities:\")\n",
        "        for idx in best_feature_set:\n",
        "            print(f\"  Index {idx}: {connectivity_list[idx]}\")\n",
        "    print(f\"Best Metrics - Accuracy: {best_metrics[0]:.4f}, AUC: {best_metrics[1]:.4f}, MCC: {best_metrics[2]:.4f}, Sensitivity: {best_metrics[3]:.4f}, Specificity: {best_metrics[4]:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Save Excel file\n",
        "    if output_dir and st_num is not None:\n",
        "        #excel_filename = os.path.join(output_dir, f'ST_{st_num}_feature_sets_metrics.xlsx')\n",
        "        excel_filename = os.path.join(output_dir, f'P_{st_num}_feature_sets_metrics.xlsx')\n",
        "        wb.save(excel_filename)\n",
        "        print(f\"Excel file saved: {excel_filename}\")\n",
        "    \n",
        "    return best_feature_set, best_metrics\n",
        "\n",
        "# Directory and file details\n",
        "connectivity = ['LPFC-->RPFC', 'LPFC-->LPMC', 'LPFC-->RPMC', 'LPFC-->SMA', 'RPFC-->LPFC', 'RPFC-->LPMC', 'RPFC-->RPMC', 'RPFC-->SMA', 'LPMC-->LPFC', 'LPMC-->RPFC',\n",
        "                'LPMC-->RPMC', 'LPMC-->SMA', 'RPMC-->LPFC', 'RPMC-->RPFC', 'RPMC-->LPMC', 'RPMC-->SMA', 'SMA-->LPFC', 'SMA-->RPFC', 'SMA-->LPMC', 'SMA-->RPMC']\n",
        "Dayx = 'D1'\n",
        "Dayy = 'Retention'\n",
        "subtask = ['ST1','ST2','ST3','ST4','ST5','ST6','ST7','ST8','ST9','ST10','ST11','ST12','ST13']\n",
        "# Loop over st_num\n",
        "for st_num in range(0, 13):\n",
        "    ###\n",
        "    dir1 =   # directory of connectivities data set1\n",
        "    dir2 =   # directory of connectivities data set2\n",
        "    output_directory = # directory to save the outputs\n",
        "    # HypothesisName = subtask[st_num]+Dayx+'vs'+Dayy+'.xlsx'\n",
        "    # print('Working on :',HypothesisName)\n",
        "    filename1 = Dayx+'_LC_ST'+subtask[st_num]+'.csv'\n",
        "    filename2 = 'Reten'+subtask[st_num]+'.csv'\n",
        "    data1 = pd.read_csv(os.path.join(dir1,filename1))\n",
        "    data1 = data1.drop('DaySubjTrial', axis = 1).values\n",
        "    data1[:,-1] = 0  # convert the class lable\n",
        "    data2 = pd.read_csv(os.path.join(dir2,filename2)).values\n",
        "    data2[:,-1] = 1  # convert the class lable\n",
        "    data = np.concatenate((data1, data2))  # join two datasets\n",
        "    data = np.delete(data,[0,6,12,18,24],1)  # delete self-causal columns from the datasets.\n",
        "\n",
        "    m,n = data.shape\n",
        "    X = normalize(data[:,0:n-1])\n",
        "    print(X.shape)\n",
        "    y = data[:,n-1]\n",
        "    print(y.shape)\n",
        "    ###\n",
        "    # y = np.array([0 if y[i] == -1 else 1 for i in range(len(y))])\n",
        "   \n",
        "    # When calling the function, pass the connectivity list, output directory, and st_num\n",
        "    best_features, best_metrics = recursive_feature_elimination(X, y, min_features=1, connectivity_list=connectivity, output_dir=output_directory, st_num=st_num+1)\n",
        "    print(\"\\nSelected features (in order of importance):\", best_features)\n",
        "    print(f\"Optimal number of features: {len(best_features)}\")\n",
        "    print(f\"Best metrics - Accuracy: {best_metrics[0]:.4f}, AUC: {best_metrics[1]:.4f}, MCC: {best_metrics[2]:.4f}, Sensitivity: {best_metrics[3]:.4f}, Specificity: {best_metrics[4]:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
